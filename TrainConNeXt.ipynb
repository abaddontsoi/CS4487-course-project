{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.datasets import ImageFolder\n",
    "import timm\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "def split_train_to_val(source_dir, val_ratio=0.2, seed=42):\n",
    "    source_dir = Path(source_dir)\n",
    "    val_dir = source_dir.parent / \"val\"\n",
    "\n",
    "    random.seed(seed)\n",
    "\n",
    "    # Define class subdirectories\n",
    "    classes = [\"0_real\", \"1_fake\"]\n",
    "\n",
    "    for class_name in classes:\n",
    "        train_class_dir = source_dir / class_name\n",
    "        val_class_dir = val_dir / class_name\n",
    "\n",
    "        if not train_class_dir.exists():\n",
    "            print(f\"Warning: {train_class_dir} does not exist. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Create validation directory\n",
    "        val_class_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Get all image files\n",
    "        image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp'}\n",
    "        files = [f for f in train_class_dir.iterdir()\n",
    "                if f.suffix.lower() in image_extensions and f.is_file()]\n",
    "\n",
    "        if len(files) == 0:\n",
    "            print(f\"No images found in {train_class_dir}\")\n",
    "            continue\n",
    "\n",
    "        # Calculate number to move\n",
    "        num_to_move = max(1, int(len(files) * val_ratio))  # at least 1 image\n",
    "        print(f\"Moving {num_to_move}/{len(files)} images from {class_name} to validation\")\n",
    "\n",
    "        # Randomly select files\n",
    "        files_to_move = random.sample(files, num_to_move)\n",
    "\n",
    "        # Move them\n",
    "        for file_path in files_to_move:\n",
    "            dest_path = val_class_dir / file_path.name\n",
    "            shutil.move(str(file_path), str(dest_path))\n",
    "            # print(f\"Moved: {file_path.name} → {dest_path}\")\n",
    "\n",
    "    print(f\"\\nDone! Validation set created at: {val_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_train_to_val('data/train', val_ratio=0.25, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import v2 as T  # ← v2 namespace\n",
    "\n",
    "train_transform = T.Compose([\n",
    "    T.Resize((384, 384)),\n",
    "    T.RandomHorizontalFlip(p=0.5),\n",
    "    T.RandomVerticalFlip(p=0.5),\n",
    "    T.RandomRotation(15),\n",
    "    T.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
    "    T.ToImage(),                   # ← replaces old ToTensor() + handles PIL→Tensor\n",
    "    T.ToDtype(torch.float32, scale=True),  # ← replaces ToTensor()'s /255\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225]),\n",
    "    T.RandomErasing(p=0.5, scale=(0.02, 0.2), ratio=(0.3, 3.3), value='random'),  # ← now works!\n",
    "])\n",
    "\n",
    "val_transform = T.Compose([\n",
    "    T.Resize((384, 384)),\n",
    "    T.ToImage(),\n",
    "    T.ToDtype(torch.float32, scale=True),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageFolder('data/train', transform=train_transform)\n",
    "val_dataset   = ImageFolder('data/val',   transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, \n",
    "                          num_workers=8, pin_memory=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=64, shuffle=False, \n",
    "                          num_workers=8, pin_memory=True)\n",
    "\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "\n",
    "# Option 1: Base fcmae pretrained (default, works on 224x224 input; fastest to load)\n",
    "# model = timm.create_model('convnextv2_huge.fcmae', pretrained=True, num_classes=2)\n",
    "\n",
    "# Option 2: fcmae pretrain + in22k fine-tune + in1k fine-tune at 512x512 (SOTA for high-res; matches your 384x384 setup well)\n",
    "# Requires timm >=0.9.2; uses larger input for better artifact detection\n",
    "model = timm.create_model('convnextv2_huge.fcmae_ft_in22k_in1k_512', pretrained=True, num_classes=2)\n",
    "\n",
    "# Option 3: Simpler in1k-pretrained (if fcmae tags fail; still strong baseline)\n",
    "# model = timm.create_model('convnextv2_huge', pretrained=True, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(weight=torch.tensor([1.0, 1.5]).to(device))  # slight weight on fakes if needed\n",
    "# OR use Label Smoothing\n",
    "# criterion = nn.CrossEntropyLoss(label_smoothing=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=3e-5, weight_decay=0.05)\n",
    "\n",
    "# Cosine annealing with warmup\n",
    "num_epochs = 30\n",
    "total_steps = len(train_loader) * num_epochs\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(optimizer, \n",
    "                                          max_lr=1e-4,\n",
    "                                          total_steps=total_steps,\n",
    "                                          pct_start=0.1,\n",
    "                                          anneal_strategy='cos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_auc = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for inputs, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}'):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            probs = torch.softmax(outputs, dim=1)[:, 1]\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            \n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "    \n",
    "    auc = roc_auc_score(all_labels, all_probs)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1} - Val AUC: {auc:.5f} - Val Acc: {acc:.5f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if auc > best_auc:\n",
    "        best_auc = auc\n",
    "        torch.save(model.state_dict(), 'best_aigc_detector.pth')\n",
    "        print(f\"New best model saved! AUC: {auc:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(all_labels, all_preds, target_names=['Real', 'Fake']))\n",
    "print(f\"Final Best Validation AUC: {best_auc:.5f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
